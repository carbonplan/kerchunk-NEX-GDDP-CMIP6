{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelizing Reference Generation with Coiled\n",
    "This notebook is used to generate the reference files for the NASA-NEX-GDDP-CMIP6 dataset. This example uses `Coiled` to spin up a large # of small workers and reduces the computation time from 8+ hours on a 32 core 256Gb RAM AWS instance to 30 minutes with 500 `t4g.small` workers. Processing this 36 TB dataset with coiled cost approximately $5 of AWS Compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "import dask\n",
    "import fsspec\n",
    "import pandas as pd\n",
    "from fsspec.implementations.reference import LazyReferenceMapper\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "from kerchunk.hdf import SingleHdf5ToZarr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the coiled cluster setup, we are trying to reduce cloud costs by: \n",
    "1. Choosing small workers `t4g.small`\n",
    "1. Choosing to use spot instances\n",
    "1. Specifying worker with ARM processing architecture, which is cheaper on AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = coiled.Cluster(\n",
    "    n_workers=500,\n",
    "    worker_vm_types=[\"t4g.small\"],\n",
    "    spot_policy=\"spot_with_fallback\",\n",
    "    arm=True,\n",
    ")\n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nasa_nex_df() -> pd.DataFrame:\n",
    "    return pd.read_csv(\n",
    "        \"s3://carbonplan-share/nasa-nex-reference/nasa_nex_formatted.csv\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _GCM_scenarios(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = pd.read_csv(\"s3://carbonplan-share/nasa-nex-reference/nasa_nex_formatted.csv\")\n",
    "    colapsed_df = (\n",
    "        df.groupby([\"GCM\", \"scenario\", \"ensemble_member\"])[\"variable\"]\n",
    "        .apply(list)\n",
    "        .reset_index()\n",
    "    )\n",
    "    colapsed_df[\"variable\"] = colapsed_df[\"variable\"].apply(lambda x: list(set(x)))\n",
    "\n",
    "    return colapsed_df\n",
    "\n",
    "\n",
    "def read_catalog_file(catalog_url):\n",
    "    return pd.read_csv(catalog_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_file_url = \"s3://carbonplan-share/nasa-nex-reference/reference_catalog_prod.csv\"\n",
    "nasa_nex_df = _nasa_nex_df()\n",
    "nasa_nex_catalog = _GCM_scenarios(nasa_nex_df)\n",
    "nasa_nex_catalog[\"ID\"] = nasa_nex_catalog[\"GCM\"] + \"_\" + nasa_nex_catalog[\"scenario\"]\n",
    "\n",
    "# If we're going to write, we can use this to check what refs exist in our catalog\n",
    "# kerchunk_ref_catalog = read_catalog_file(catalog_file_url)\n",
    "# missing_refs_df = nasa_nex_catalog.merge(kerchunk_ref_catalog, how='outer', on='ID',indicator=True).query('_merge != \"both\"')[['GCM', 'scenario', 'ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we're not writing, we can use this for testing\n",
    "missing_refs_df = nasa_nex_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_refs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_read = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n",
    "so = dict(mode=\"rb\", anon=True, default_fill_cache=False, default_cache_type=\"first\")\n",
    "\n",
    "\n",
    "def build_reference_catalog(catalog_file_url: str):\n",
    "    ref_list = [\n",
    "        \"s3://\" + ref\n",
    "        for ref in fs_read.ls(\n",
    "            \"s3://carbonplan-share/nasa-nex-reference/references_prod/\"\n",
    "        )\n",
    "    ]\n",
    "    ref_list.remove(\"s3://carbonplan-share/nasa-nex-reference/references_prod/\")\n",
    "    ref_df = pd.DataFrame({\"ID\": None, \"url\": ref_list})\n",
    "    ref_df[\"ID\"] = ref_df[\"url\"].str.split(\"/\", expand=True)[5]\n",
    "    ref_df.to_csv(catalog_file_url, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def combine_refs(refs, outpath):\n",
    "    fs = fsspec.filesystem(\"s3\")\n",
    "\n",
    "    if fs.exists(outpath):\n",
    "        fs.rm(outpath, recursive=True)\n",
    "    fs.makedir(outpath)\n",
    "    out = LazyReferenceMapper.create(10000, outpath, fs)\n",
    "    mzz = MultiZarrToZarr(\n",
    "        refs,\n",
    "        remote_protocol=\"s3\",\n",
    "        concat_dims=[\"time\"],\n",
    "        identical_dims=[\"lat\", \"lon\"],\n",
    "        out=out,\n",
    "    ).translate()\n",
    "    out.flush()\n",
    "\n",
    "    return mzz\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def generate_json_reference(fil):\n",
    "    with fs_read.open(fil, **so) as infile:\n",
    "        h5chunks = SingleHdf5ToZarr(infile, fil, inline_threshold=300)\n",
    "        return h5chunks.translate()\n",
    "\n",
    "\n",
    "def gen_all_refs(row):\n",
    "    GCM = row[\"GCM\"]\n",
    "    scenario = row[\"scenario\"]\n",
    "    target_root = \"s3://carbonplan-share/nasa-nex-reference/references_prod/\"\n",
    "    store_name = f\"{GCM}_{scenario}\"\n",
    "    output_file_name = \"reference.parquet\"\n",
    "    outpath = target_root + store_name + \"/\" + output_file_name\n",
    "\n",
    "    file_pattern = nasa_nex_df.query(f\"GCM == '{GCM}'  & scenario == '{scenario}'\")\n",
    "    refs = [generate_json_reference(fil) for fil in file_pattern[\"url\"].to_list()]\n",
    "\n",
    "    mzz = combine_refs(refs=refs, outpath=outpath)\n",
    "    return mzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_list = [row for _, row in missing_refs_df.iterrows()]\n",
    "tasks_alt = [(gen_all_refs)(row) for row in row_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.compute(tasks_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Catalog\n",
    "This optional section will call build_reference_catalog, which is a function to scan the carbonplan s3 storage and update the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_reference_catalog(catalog_file_url)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
