{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison using kerchunk / datatree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatree import DataTree\n",
    "from datatree import map_over_subtree\n",
    "import dask.bag as db\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import dask\n",
    "import xclim\n",
    "import thermofeel as tf\n",
    "import numpy as np\n",
    "from distributed import Client\n",
    "from fsspec.implementations.reference import ReferenceFileSystem\n",
    "import kerchunk\n",
    "import coiled\n",
    "from utils import wbgt, load_elev, adjust_pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(n_workers=10)\n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the reference catalog into a Pandas DataFrame\n",
    "cat_df = pd.read_csv(\n",
    "    \"s3://carbonplan-share/nasa-nex-reference/reference_catalog_nested.csv\"\n",
    ")\n",
    "# Select only ssp245 && HISTORICAL!\n",
    "ssp245_historical_catalog = cat_df[cat_df[\"ID\"].str.contains(\"ssp245|historical\")]\n",
    "\n",
    "ssp245_historical_catalog = ssp245_historical_catalog.iloc[0:20]\n",
    "# Convert the DataFrame into a dictionary\n",
    "catalog = ssp245_historical_catalog.set_index(\"ID\").T.to_dict(\"records\")[0]\n",
    "\n",
    "# invalid? float is not iterable - this GCM ref is invalid\n",
    "# del catalog[\"GISS-E2-1-G/ssp245\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ref_ds(gcm_scenario: str, url: str):\n",
    "    storage_options = {\n",
    "        \"remote_protocol\": \"s3\",\n",
    "        \"target_protocol\": \"s3\",\n",
    "        \"lazy\": True,\n",
    "        \"skip_instance_cache\": True,\n",
    "    }  # options passed to fsspec\n",
    "    open_dataset_options = {\"chunks\": {}}  # opens passed to xarray\n",
    "    ds = xr.open_dataset(\n",
    "        url,\n",
    "        engine=\"kerchunk\",\n",
    "        storage_options=storage_options,\n",
    "        open_dataset_options=open_dataset_options,\n",
    "    )\n",
    "\n",
    "    if set(\n",
    "        [\n",
    "            \"huss\",\n",
    "            \"tasmax\",\n",
    "            \"tas\",\n",
    "        ]\n",
    "    ).issubset(set(list(ds))):\n",
    "        ds = ds[[\"huss\", \"tasmax\", \"tas\"]]\n",
    "        # adding the gcm/scenario combo to attrs for later down the pipeline\n",
    "        ds.attrs[\"gcm_scenario\"] = gcm_scenario\n",
    "        return {gcm_scenario: ds}\n",
    "\n",
    "\n",
    "# convert catalog dict to tuples for dask bag starmap\n",
    "catalog_tuple_list = list(catalog.items())\n",
    "# Create a bag from tuple catalog\n",
    "cat_bag = db.from_sequence(catalog_tuple_list)\n",
    "# apply load_ref_ds to catalog tuple bag\n",
    "task_bag = filter(None, cat_bag.starmap(load_ref_ds).compute())\n",
    "\n",
    "# convert list of dicts to dict\n",
    "catalog_computed = {k: v for element in task_bag for k, v in element.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DataTree.from_dict(catalog_computed)\n",
    "elev = load_elev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_wbgt(ds):\n",
    "    ds = ds.to_dataset()\n",
    "    ds = ds.isel(time=slice(0, 365))\n",
    "    # calculate elevation-adjusted pressure\n",
    "    ds[\"ps\"] = xr.apply_ufunc(adjust_pressure, ds[\"tas\"], elev, dask=\"allowed\").rename(\n",
    "        {\"elevation\": \"ps\"}\n",
    "    )[\"ps\"]\n",
    "    ds[\"ps\"].attrs[\"units\"] = \"Pa\"\n",
    "    ds[\"hurs\"] = xclim.indices.relative_humidity(\n",
    "        tas=ds[\"tasmax\"], huss=ds[\"huss\"], ps=ds[\"ps\"]\n",
    "    )\n",
    "    ds[\"tasmax\"].attrs = {}\n",
    "\n",
    "    # windspeed assumption of 0.5 m/s (approximating shaded/indoor\n",
    "    # conditions)\n",
    "    ds[\"sfcWind\"] = (ds[\"tas\"] - ds[\"tas\"]) + 0.5\n",
    "    ds[\"WBT\"] = tf.thermofeel.calculate_wbt(ds[\"tasmax\"] - 273.15, ds[\"hurs\"])\n",
    "\n",
    "    ds[\"BGT\"] = tf.thermofeel.calculate_bgt(ds[\"tasmax\"], ds[\"tasmax\"], ds[\"sfcWind\"])\n",
    "    ds[\"WBGT\"] = wbgt(ds[\"WBT\"], ds[\"BGT\"], ds[\"tasmax\"] - 273.15)\n",
    "    ds[\"WBGT\"].attrs[\"units\"] = \"degC\"\n",
    "    ds = ds[[\"WBGT\"]]\n",
    "    output = (\n",
    "        f\"s3://carbonplan-scratch/TEMP_NASA_NEX/wbgt-shade-\"\n",
    "        f\"gridded/years/{ds.attrs['gcm_scenario']}.zarr\"\n",
    "    )\n",
    "\n",
    "    return ds.to_zarr(output, consolidated=True, compute=True, mode=\"w\")\n",
    "\n",
    "\n",
    "ds_list = [ds for ds in dt.leaves if ds.dims]\n",
    "\n",
    "bag = db.from_sequence(ds_list, npartitions=len(ds_list)).map(calc_wbgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
